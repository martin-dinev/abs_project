{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T07:29:12.147244800Z",
     "start_time": "2023-07-11T07:29:12.139850400Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Input, Reshape, Lambda, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T07:29:12.284341800Z",
     "start_time": "2023-07-11T07:29:12.274469600Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array( tuple(-1 if e == 2 else int(e) for e in observation['board']))\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (0.5 if _reward == 1 else 0 if _reward == 0 else -0.3) if _reward is not None else -30\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T07:29:12.518067100Z",
     "start_time": "2023-07-11T07:29:12.417231300Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "r = Reshape((42,))(i)\n",
    "y = Lambda(lambda e: e[:, :7])(r)\n",
    "x = Dense(42*4, activation='sigmoid')(r)\n",
    "x = Dense(42*4, activation='sigmoid')(x)\n",
    "x = Dense(42*4, activation='sigmoid')(x)\n",
    "x = Dense(42*4, activation='sigmoid')(x)\n",
    "x = Dense(7, activation='sigmoid')(x)\n",
    "o = Concatenate()([x, y])\n",
    "o = Dense(7, activation='sigmoid')(o)\n",
    "o = Dense(7, activation='linear')(o)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T07:29:13.279392Z",
     "start_time": "2023-07-11T07:29:12.549090200Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.1)\n",
    "policy = LinearAnnealedPolicy(policy, attr='eps', value_max=0.1, value_min=0.01, value_test=0.001, nb_steps=100000)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(learning_rate=0.01), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def mean_reward(_rewards):\n",
    "    _rewards = [r[0] for r in _rewards]\n",
    "    wins = sum(1 for r in _rewards if r == 1)\n",
    "    losses = sum(1 for r in _rewards if r == -1)\n",
    "    mistakes = sum(1 for r in _rewards if r is None)\n",
    "    opponent_mistakes = sum(1 for r in _rewards if r == 0)\n",
    "    return \"W: \" + str(wins) + \"; L: \" + str(losses) + \"; M: \" + str(mistakes) + \"; O: \" + str(opponent_mistakes)\n",
    "from kaggle_environments import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T07:29:13.286270100Z",
     "start_time": "2023-07-11T07:29:13.283333600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: -0.2484\n",
      "1728 episodes - episode_reward: -1.437 [-30.000, 0.500] - loss: 3.978 - mae: 2.073 - mean_q: -0.870 - mean_eps: 0.095\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 177s 18ms/step - reward: -0.1501\n",
      "1714 episodes - episode_reward: -0.876 [-30.000, 0.500] - loss: 1.674 - mae: 1.521 - mean_q: -0.083 - mean_eps: 0.087\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: -0.1575\n",
      "1690 episodes - episode_reward: -0.932 [-30.000, 0.500] - loss: 1.751 - mae: 1.722 - mean_q: 0.345 - mean_eps: 0.078\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 173s 17ms/step - reward: -0.1442\n",
      "1711 episodes - episode_reward: -0.843 [-30.000, 0.500] - loss: 1.738 - mae: 1.811 - mean_q: 0.340 - mean_eps: 0.069\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 173s 17ms/step - reward: -0.0433\n",
      "1708 episodes - episode_reward: -0.254 [-30.000, 0.500] - loss: 1.725 - mae: 1.785 - mean_q: 0.386 - mean_eps: 0.060\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 175s 18ms/step - reward: -0.0821\n",
      "1737 episodes - episode_reward: -0.473 [-30.000, 0.500] - loss: 1.657 - mae: 1.693 - mean_q: 0.415 - mean_eps: 0.051\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 177s 18ms/step - reward: -0.0452\n",
      "1752 episodes - episode_reward: -0.258 [-30.000, 0.500] - loss: 1.323 - mae: 1.595 - mean_q: 0.495 - mean_eps: 0.042\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 7386s 739ms/step - reward: -0.0502\n",
      "1751 episodes - episode_reward: -0.286 [-30.000, 0.500] - loss: 1.264 - mae: 1.519 - mean_q: 0.443 - mean_eps: 0.033\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -0.0598\n",
      "1790 episodes - episode_reward: -0.334 [-30.000, 0.500] - loss: 1.325 - mae: 1.599 - mean_q: 0.559 - mean_eps: 0.024\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: -0.0198\n",
      "done, took 1777.367 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"bigger\"\n",
    "env = make(\"connectx\", debug=False)\n",
    "trainer = env.train([\"random\", None])\n",
    "agent.fit(trainer, nb_steps=100000, visualize=False, verbose=1)\n",
    "agent.save_weights(f'{save_name}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T09:58:50.947117Z",
     "start_time": "2023-07-11T07:29:13.290709800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 691; L: 13; M: 296; O: 0\n"
     ]
    }
   ],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=1000)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T10:07:49.125930400Z",
     "start_time": "2023-07-11T10:07:11.405148100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
