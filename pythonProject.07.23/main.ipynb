{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:33:00.873345800Z",
     "start_time": "2023-07-04T12:33:00.868347300Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Dropout, Input, Reshape\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:51:51.269923400Z",
     "start_time": "2023-07-04T12:51:51.253742700Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array(observation['board'])\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (10 if _reward == 1 else 0 if _reward == 0 else -10) if _reward is not None else -10\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:51:52.032895Z",
     "start_time": "2023-07-04T12:51:51.832320900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.dense.Dense object at 0x0000025E4A088A30>\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "x = Reshape((42,))(i)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(24, activation='linear')(x)\n",
    "x = Dense(24, activation='linear')(x)\n",
    "o = Dense(7, activation='leaky_relu')(x)\n",
    "model = Model(inputs=i, outputs=o)\n",
    "print(model.layers[-2])\n",
    "print(model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:51:53.588295100Z",
     "start_time": "2023-07-04T12:51:52.601491500Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.2)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T07:24:42.630933Z",
     "start_time": "2023-07-04T07:13:52.459334800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 128s 13ms/step - reward: 0.5338\n",
      "1160 episodes - episode_reward: 4.599 [-8.000, 12.000] - loss: 0.920 - mae: 2.547 - mean_q: 3.379\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 126s 13ms/step - reward: 0.5716\n",
      "1133 episodes - episode_reward: 5.046 [-6.500, 12.000] - loss: 0.779 - mae: 3.146 - mean_q: 3.909\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 128s 13ms/step - reward: 0.5439\n",
      "1130 episodes - episode_reward: 4.815 [-7.500, 12.500] - loss: 0.754 - mae: 3.319 - mean_q: 4.060\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 0.5457\n",
      "1131 episodes - episode_reward: 4.824 [-6.500, 11.500] - loss: 0.748 - mae: 3.309 - mean_q: 4.005\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 135s 14ms/step - reward: 0.5249\n",
      "done, took 650.001 seconds\n"
     ]
    }
   ],
   "source": [
    "env = make(\"connectx\", debug=False)\n",
    "trainer = env.train([None, \"random\"])\n",
    "agent.fit(trainer, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "agent.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T07:35:46.711114500Z",
     "start_time": "2023-07-04T07:24:47.778923900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 131s 13ms/step - reward: 0.5540\n",
      "1172 episodes - episode_reward: 4.725 [-6.500, 12.000] - loss: 0.776 - mae: 3.367 - mean_q: 4.028\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 0.5342\n",
      "1135 episodes - episode_reward: 4.705 [-7.500, 11.500] - loss: 0.793 - mae: 3.362 - mean_q: 4.013\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 132s 13ms/step - reward: 0.5030\n",
      "1068 episodes - episode_reward: 4.709 [-8.000, 12.000] - loss: 0.815 - mae: 3.389 - mean_q: 4.047\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 132s 13ms/step - reward: 0.5139\n",
      "1095 episodes - episode_reward: 4.695 [-7.000, 12.000] - loss: 0.831 - mae: 3.384 - mean_q: 4.046\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 131s 13ms/step - reward: 0.5131\n",
      "done, took 658.886 seconds\n"
     ]
    }
   ],
   "source": [
    "agent.fit(trainer, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "agent.save_weights('dqn_weights_2.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T07:50:04.343118Z",
     "start_time": "2023-07-04T07:38:48.818984400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 135s 14ms/step - reward: 0.4859\n",
      "1131 episodes - episode_reward: 4.293 [-7.500, 11.500] - loss: 0.841 - mae: 3.287 - mean_q: 4.080\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 135s 13ms/step - reward: 0.4907\n",
      "1200 episodes - episode_reward: 4.090 [-7.000, 12.500] - loss: 0.895 - mae: 3.204 - mean_q: 3.930\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 131s 13ms/step - reward: 0.4833\n",
      "1188 episodes - episode_reward: 4.070 [-7.500, 11.500] - loss: 0.897 - mae: 3.091 - mean_q: 3.772\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.4868\n",
      "1188 episodes - episode_reward: 4.095 [-7.500, 12.000] - loss: 0.863 - mae: 3.031 - mean_q: 3.690\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 134s 13ms/step - reward: 0.4832\n",
      "done, took 675.314 seconds\n"
     ]
    }
   ],
   "source": [
    "agent.load_weights('dqn_weights_2.h5f')\n",
    "agent.fit(trainer, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "agent.save_weights('dqn_weights_3.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-04T08:27:49.796587700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 632s 63ms/step - reward: 0.1283\n",
      "done, took 632.274 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 596s 60ms/step - reward: 0.1421\n",
      "done, took 596.335 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.1563\n",
      "done, took 578.387 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 577s 58ms/step - reward: 0.1431\n",
      "done, took 577.228 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.1421\n",
      "done, took 579.499 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 598s 60ms/step - reward: 0.1439\n",
      "done, took 598.367 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 592s 59ms/step - reward: 0.1388\n",
      "done, took 592.345 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 584s 58ms/step - reward: 0.1405\n",
      "done, took 583.538 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 600s 60ms/step - reward: 0.1467\n",
      "done, took 599.906 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 593s 59ms/step - reward: 0.1640\n",
      "done, took 592.904 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 594s 59ms/step - reward: 0.1620\n",
      "done, took 593.691 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 600s 60ms/step - reward: 0.1646\n",
      "done, took 599.556 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 609s 61ms/step - reward: 0.1361\n",
      "done, took 608.937 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 619s 62ms/step - reward: 0.1661\n",
      "done, took 618.850 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 612s 61ms/step - reward: 0.1645\n",
      "done, took 612.406 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 8452/10000 [========================>.....] - ETA: 1:32 - reward: 0.1649"
     ]
    }
   ],
   "source": [
    "for c in range(3,23):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    if c>0:\n",
    "        agent.load_weights(f'dqn_weights_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=10000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'dqn_weights_{c+1}.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:28:32.505971Z",
     "start_time": "2023-07-04T12:28:32.489143200Z"
    }
   },
   "outputs": [],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "env.play([kaggle_agent, None], width=500, height=450, fps=10)\n",
    "# env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T13:47:00.304584500Z",
     "start_time": "2023-07-04T13:18:25.632130500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 5000/10000 [==============>...............] - ETA: 5:50 - reward: -0.7560done, took 350.140 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 4998/10000 [=============>................] - ETA: 5:41 - reward: -0.6643done, took 341.624 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 5000/10000 [==============>...............] - ETA: 5:35 - reward: -0.6840done, took 335.882 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 5000/10000 [==============>...............] - ETA: 5:36 - reward: -0.6320done, took 336.635 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 5000/10000 [==============>...............] - ETA: 5:31 - reward: -0.6520done, took 331.924 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  135/10000 [..............................] - ETA: 9:59 - reward: -0.7407 done, took 8.299 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 15:57 - reward: 0.0000e+00done, took 0.379 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    2/10000 [..............................] - ETA: 20:25 - reward: 0.0000e+00done, took 0.290 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 16:36 - reward: 0.0000e+00done, took 0.378 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 16:14 - reward: 0.0000e+00done, took 0.414 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 16:58 - reward: 0.0000e+00done, took 0.388 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    9/10000 [..............................] - ETA: 14:38 - reward: 0.0000e+00done, took 0.876 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   16/10000 [..............................] - ETA: 8:21 - reward: -0.6250    done, took 0.961 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   10/10000 [..............................] - ETA: 10:24 - reward: 0.0000e+00done, took 0.807 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    6/10000 [..............................] - ETA: 18:16 - reward: 0.0000e+00done, took 0.714 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    9/10000 [..............................] - ETA: 15:45 - reward: -1.1111done, took 0.898 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    8/10000 [..............................] - ETA: 14:29 - reward: 0.0000e+00done, took 0.764 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    8/10000 [..............................] - ETA: 16:05 - reward: -1.2500done, took 0.916 seconds\n",
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   11/10000 [..............................] - ETA: 13:47 - reward: -0.9091   done, took 1.030 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd\"\n",
    "for c in range(1,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=5000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:50 - reward: -0.4440done, took 156.897 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 7:34 - reward: -0.2842done, took 151.545 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:46 - reward: -0.3400done, took 155.730 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:38 - reward: -0.3760done, took 153.065 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:28 - reward: -0.2680done, took 149.445 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 8:04 - reward: -0.2042done, took 161.427 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 7:39 - reward: -0.2402done, took 153.032 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:17 - reward: -0.1760done, took 145.758 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:25 - reward: -0.1360done, took 148.556 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   63/10000 [..............................] - ETA: 7:24 - reward: 0.1587done, took 2.980 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    2/10000 [..............................] - ETA: 21:07 - reward: 0.0000e+00done, took 0.314 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "done, took 0.365 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "done, took 0.280 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 32:40 - reward: 0.0000e+00done, took 0.298 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd\"\n",
    "agent.policy = EpsGreedyQPolicy(0.05)\n",
    "for c in range(6,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2500, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T14:12:52.956930700Z",
     "start_time": "2023-07-04T13:49:52.117416700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 7:07 - reward: -0.0721done, took 142.469 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:07 - reward: -0.0720done, took 142.519 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:08 - reward: -0.1280done, took 142.843 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2497/10000 [======>.......................] - ETA: 7:13 - reward: -0.0561done, took 144.540 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2499/10000 [======>.......................] - ETA: 7:04 - reward: 0.0360done, took 141.459 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd\"\n",
    "agent.policy = EpsGreedyQPolicy(0.025)\n",
    "for c in range(15,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2500, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T14:25:15.509369600Z",
     "start_time": "2023-07-04T14:13:21.278029900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1999/10000 [====>.........................] - ETA: 7:40 - reward: 0.0850done, took 115.193 seconds\n",
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2000/10000 [=====>........................] - ETA: 7:37 - reward: -0.0300done, took 114.517 seconds\n",
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2000/10000 [=====>........................] - ETA: 7:43 - reward: -0.0350done, took 115.953 seconds\n",
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1999/10000 [====>.........................] - ETA: 7:22 - reward: -0.0500done, took 110.760 seconds\n",
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2000/10000 [=====>........................] - ETA: 7:29 - reward: -0.0050done, took 112.417 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd\"\n",
    "agent.policy = EpsGreedyQPolicy(0.0125)\n",
    "for c in range(20,25):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2000, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T14:35:28.212019900Z",
     "start_time": "2023-07-04T14:25:59.016413700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 9:00 - reward: -0.3403done, took 179.968 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 9:05 - reward: -0.2400done, took 182.092 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2499/10000 [======>.......................] - ETA: 8:48 - reward: -0.2601done, took 176.276 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2500/10000 [======>.......................] - ETA: 7:45 - reward: -0.2840done, took 155.157 seconds\n",
      "Training for 2500 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2498/10000 [======>.......................] - ETA: 7:39 - reward: -0.3082done, took 153.125 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd\"\n",
    "agent.policy = EpsGreedyQPolicy(0.1)\n",
    "for c in range(25,30):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2500, visualize=False, verbose=1, nb_max_episode_steps=1000)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T15:11:34.186869100Z",
     "start_time": "2023-07-04T14:57:27.141573600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_s2 failed: No module named 'pettingzoo'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:40:52.413428Z",
     "start_time": "2023-07-06T09:40:52.105895400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:40:52.468824600Z",
     "start_time": "2023-07-06T09:40:52.414440800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "trainer = env.train([None, \"negamax\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:40:58.135237Z",
     "start_time": "2023-07-06T09:40:58.104047100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'remainingOverageTime': 60,\n 'step': 0,\n 'board': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'mark': 1}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:41:02.042666300Z",
     "start_time": "2023-07-06T09:41:01.988446200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'action': 0,\n  'reward': 0,\n  'info': {},\n  'observation': {'remainingOverageTime': 60,\n   'step': 0,\n   'board': [0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0],\n   'mark': 1},\n  'status': 'ACTIVE'},\n {'action': 0,\n  'reward': 0,\n  'info': {},\n  'observation': {'remainingOverageTime': 60, 'mark': 2},\n  'status': 'INACTIVE'}]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:41:58.216584100Z",
     "start_time": "2023-07-06T09:41:58.163395100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Action: Invalid column: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, 0)"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = env.run([ lambda _,__:0,\"random\"])\n",
    "[(x[0]['reward'], x[1]['reward']) for x in ep][-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:47:15.846629300Z",
     "start_time": "2023-07-06T09:47:15.794747300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (0, 0),\n (1, -1)]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T09:45:36.739010400Z",
     "start_time": "2023-07-06T09:45:36.708980500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
