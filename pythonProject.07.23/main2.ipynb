{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Dropout, Input, Reshape\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T11:07:46.849921100Z",
     "start_time": "2023-07-06T11:07:46.834166900Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array(observation['board'])\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (3 if _reward == 1 else -0.1 if _reward == 0 else -7) if _reward is not None else -21\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T11:07:47.053993500Z",
     "start_time": "2023-07-06T11:07:46.851922800Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "x = Reshape((42,))(i)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(24, activation='linear')(x)\n",
    "o = Dense(7, activation='leaky_relu')(x)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T11:07:48.035018800Z",
     "start_time": "2023-07-06T11:07:47.053993500Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.15)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1003s 100ms/step - reward: -1.1673\n",
      "done, took 1003.623 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 1005s 101ms/step - reward: -1.0867\n",
      "done, took 1005.525 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 953s 95ms/step - reward: -1.1208\n",
      "done, took 953.171 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 899s 90ms/step - reward: -1.0736\n",
      "done, took 899.107 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 903s 90ms/step - reward: -1.1147\n",
      "done, took 903.041 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 950s 95ms/step - reward: -1.1227\n",
      "done, took 950.241 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd2\"\n",
    "for c in range(4,10):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"negamax\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2/(c+1))\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=10000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T12:43:16.026513200Z",
     "start_time": "2023-07-06T11:07:59.435685300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 162s 16ms/step - reward: 0.0478\n",
      "done, took 162.196 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 165s 17ms/step - reward: 0.1095\n",
      "done, took 165.024 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: 0.0737\n",
      "done, took 169.146 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0533\n",
      "done, took 169.575 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 168s 17ms/step - reward: 0.1598\n",
      "done, took 167.997 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: 0.1802\n",
      "done, took 170.581 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 170s 17ms/step - reward: 0.2200\n",
      "done, took 170.293 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: 0.1707\n",
      "done, took 172.321 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0934\n",
      "done, took 171.714 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: 0.1726\n",
      "done, took 171.316 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"dqn_weights_dd2\"\n",
    "for c in range(10,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"random\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2/(c+1))\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=10000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T14:12:43.369439300Z",
     "start_time": "2023-07-06T13:44:32.171389100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "save_name = \"dqn_weights_dd2\"\n",
    "agent.load_weights(f'{save_name}_{20}.h5f')\n",
    "def kaggle_agent(observation, _):\n",
    "    return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T15:47:23.832007700Z",
     "start_time": "2023-07-06T15:47:23.720359100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[[1, -1],\n [-1, 1],\n [1, -1],\n [None, 0],\n [1, -1],\n [1, -1],\n [None, 0],\n [1, -1],\n [1, -1],\n [1, -1]]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "env = make(\"connectx\", debug=True)\n",
    "evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T15:47:37.132321500Z",
     "start_time": "2023-07-06T15:47:36.457395800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
