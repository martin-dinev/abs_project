{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T11:28:48.134041500Z",
     "start_time": "2023-07-11T11:28:48.121743500Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Input, Reshape, Lambda, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T11:28:48.345100600Z",
     "start_time": "2023-07-11T11:28:48.341192700Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array( tuple(-1 if e == 2 else int(e) for e in observation['board']))\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (1 if _reward == 1 else 0 if _reward == 0 else -1) if _reward is not None else -5\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T11:28:48.516959800Z",
     "start_time": "2023-07-11T11:28:48.464222Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "r = Reshape((42,))(i)\n",
    "x = Dense(42, activation='leaky_relu')(r)\n",
    "x = Dense(7, activation='leaky_relu')(x)\n",
    "o = Dense(7, activation='linear')(x)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T11:29:51.136029600Z",
     "start_time": "2023-07-11T11:29:50.611223200Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "policy = LinearAnnealedPolicy(policy, attr='eps', value_max=0.1, value_min=0.005, value_test=0.001, nb_steps=50000)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=1000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=3e2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(learning_rate=0.01), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def mean_reward(_rewards):\n",
    "    _rewards = [r[0] for r in _rewards]\n",
    "    wins = sum(1 for r in _rewards if r == 1)\n",
    "    losses = sum(1 for r in _rewards if r == -1)\n",
    "    mistakes = sum(1 for r in _rewards if r is None)\n",
    "    opponent_mistakes = sum(1 for r in _rewards if r == 0)\n",
    "    return \"W: \" + str(wins) + \"; L: \" + str(losses) + \"; M: \" + str(mistakes) + \"; O: \" + str(opponent_mistakes)\n",
    "from kaggle_environments import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T11:29:51.193442200Z",
     "start_time": "2023-07-11T11:29:51.189536700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: -0.0167\n",
      "1493 episodes - episode_reward: -0.112 [-5.000, 1.000] - loss: 0.119 - mae: 0.586 - mean_q: 0.472 - mean_eps: 0.090\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: -0.0055\n",
      "1565 episodes - episode_reward: -0.035 [-5.000, 1.000] - loss: 0.114 - mae: 0.663 - mean_q: 0.420 - mean_eps: 0.072\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0302\n",
      "1602 episodes - episode_reward: 0.189 [-5.000, 1.000] - loss: 0.103 - mae: 0.675 - mean_q: 0.495 - mean_eps: 0.053\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 154s 15ms/step - reward: 0.0349\n",
      "1603 episodes - episode_reward: 0.218 [-5.000, 1.000] - loss: 0.093 - mae: 0.651 - mean_q: 0.568 - mean_eps: 0.034\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 157s 16ms/step - reward: 0.0504\n",
      "done, took 752.460 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"smaller\"\n",
    "env = make(\"connectx\", debug=False)\n",
    "trainer = env.train([\"random\", None])\n",
    "agent.fit(trainer, nb_steps=50000, visualize=False, verbose=1)\n",
    "agent.save_weights(f'{save_name}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T11:42:24.835290800Z",
     "start_time": "2023-07-11T11:29:52.225811900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 734; L: 11; M: 255; O: 0\n"
     ]
    }
   ],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=1000)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T12:31:32.402281400Z",
     "start_time": "2023-07-11T12:30:54.533417500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 712; L: 8; M: 280; O: 0\n"
     ]
    }
   ],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return 1\n",
    "print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=1000)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T11:17:25.793357600Z",
     "start_time": "2023-07-11T11:16:53.066532100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
