{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T15:45:12.613164600Z",
     "start_time": "2023-07-07T15:45:12.597163600Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Input, Reshape, Lambda, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T16:12:12.954766300Z",
     "start_time": "2023-07-07T16:12:12.946923200Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array(observation['board'])\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (1 if _reward == 1 else 0.5 if _reward == 0 else 0) if _reward is not None else -84\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T15:45:14.951462200Z",
     "start_time": "2023-07-07T15:45:14.836407900Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "r = Reshape((42,))(i)\n",
    "y = Lambda(lambda e: e[:, :7])(r)\n",
    "x = Dense(42, activation='leaky_relu')(r)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dense(7, activation='linear')(x)\n",
    "o = Concatenate()([x, y])\n",
    "o = Dense(7, activation='leaky_relu')(o)\n",
    "o = Dense(7, activation='linear')(o)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T15:45:17.682656600Z",
     "start_time": "2023-07-07T15:45:17.001798900Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.1)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(learning_rate=0.01), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def mean_reward(_rewards):\n",
    "    _rewards = [r[0] for r in _rewards]\n",
    "    wins = sum(1 for r in _rewards if r == 1)\n",
    "    losses = sum(1 for r in _rewards if r == -1)\n",
    "    mistakes = sum(1 for r in _rewards if r is None)\n",
    "    opponent_mistakes = sum(1 for r in _rewards if r == 0)\n",
    "    return \"W: \" + str(wins) + \"; L: \" + str(losses) + \"; M: \" + str(mistakes) + \"; O: \" + str(opponent_mistakes)\n",
    "from kaggle_environments import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:09:03.321047500Z",
     "start_time": "2023-07-07T16:09:03.305522100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 140s 14ms/step - reward: -0.2117\n",
      "done, took 140.125 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 142s 14ms/step - reward: -0.0271\n",
      "done, took 141.563 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.0503\n",
      "done, took 141.023 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: 0.0264\n",
      "done, took 143.055 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.1998\n",
      "done, took 143.669 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.3151\n",
      "done, took 144.535 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.3220\n",
      "done, took 141.491 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: 0.3892\n",
      "done, took 142.560 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: 0.3991\n",
      "done, took 142.547 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: 0.4803\n",
      "done, took 142.566 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"first_layer\"\n",
    "for c in range(0,10):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"random\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2-c*0.02)\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=10000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:09:03.320039900Z",
     "start_time": "2023-07-07T15:45:19.257421300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 73; L: 2; M: 25; O: 0\n",
      "W: 77; L: 0; M: 23; O: 0\n",
      "W: 67; L: 5; M: 28; O: 0\n",
      "W: 77; L: 1; M: 22; O: 0\n",
      "W: 81; L: 0; M: 19; O: 0\n",
      "W: 71; L: 2; M: 27; O: 0\n",
      "W: 72; L: 1; M: 27; O: 0\n",
      "W: 73; L: 0; M: 27; O: 0\n",
      "W: 78; L: 1; M: 21; O: 0\n",
      "W: 69; L: 1; M: 30; O: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    save_name = \"first_layer\"\n",
    "    agent.load_weights(f'{save_name}_{i+1}.h5f')\n",
    "    def kaggle_agent(observation, _):\n",
    "        return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=100)))\n",
    "    # print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"negamax\"], num_episodes=20)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:11:10.288015800Z",
     "start_time": "2023-07-07T16:10:35.728242200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Didn't execute below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_name = \"first_layer\"\n",
    "for c in range(10,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"negamax\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2-c*0.02)\n",
    "    agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10, 20):\n",
    "    save_name = \"first_layer\"\n",
    "    agent.load_weights(f'{save_name}_{20}.h5f')\n",
    "    def kaggle_agent(observation, _):\n",
    "        return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=20)))\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"negamax\"], num_episodes=20)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
