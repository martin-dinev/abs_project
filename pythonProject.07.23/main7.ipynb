{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T10:18:57.366090300Z",
     "start_time": "2023-07-11T10:18:57.354303300Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Input, Reshape, Lambda, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T10:18:57.567508700Z",
     "start_time": "2023-07-11T10:18:57.555163Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array( tuple(-1 if e == 2 else int(e) for e in observation['board']))\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (1 if _reward == 1 else 0 if _reward == 0 else -0.3) if _reward is not None else -35\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T10:18:57.830373600Z",
     "start_time": "2023-07-11T10:18:57.721814800Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "r = Reshape((42,))(i)\n",
    "y = Lambda(lambda e: e[:, :7])(r)\n",
    "x = Dense(42*7, activation='leaky_relu')(r)\n",
    "x = Dense(42*7, activation='leaky_relu')(x)\n",
    "x = Dense(42*7, activation='leaky_relu')(x)\n",
    "x = Dense(42*7, activation='leaky_relu')(x)\n",
    "x = Dense(7, activation='leaky_relu')(x)\n",
    "o = Concatenate()([x, y])\n",
    "o = Dense(7, activation='leaky_relu')(o)\n",
    "o = Dense(7, activation='linear')(o)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T10:18:59.082174500Z",
     "start_time": "2023-07-11T10:18:58.180577500Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.1)\n",
    "policy = LinearAnnealedPolicy(policy, attr='eps', value_max=0.5, value_min=0.005, value_test=0.001, nb_steps=100000)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(learning_rate=0.02), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def mean_reward(_rewards):\n",
    "    _rewards = [r[0] for r in _rewards]\n",
    "    wins = sum(1 for r in _rewards if r == 1)\n",
    "    losses = sum(1 for r in _rewards if r == -1)\n",
    "    mistakes = sum(1 for r in _rewards if r is None)\n",
    "    opponent_mistakes = sum(1 for r in _rewards if r == 0)\n",
    "    return \"W: \" + str(wins) + \"; L: \" + str(losses) + \"; M: \" + str(mistakes) + \"; O: \" + str(opponent_mistakes)\n",
    "from kaggle_environments import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T10:18:59.087642800Z",
     "start_time": "2023-07-11T10:18:59.080218300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 173s 17ms/step - reward: -0.9713\n",
      "1165 episodes - episode_reward: -8.337 [-35.000, 1.000] - loss: 405236677.089 - mae: 7188.017 - mean_q: 10851.977 - mean_eps: 0.475\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 175s 18ms/step - reward: -0.9523\n",
      "1127 episodes - episode_reward: -8.450 [-35.000, 1.000] - loss: 423229.491 - mae: 1176.023 - mean_q: 1680.019 - mean_eps: 0.426\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 182s 18ms/step - reward: -1.0050\n",
      "1175 episodes - episode_reward: -8.553 [-35.000, 1.000] - loss: 40756296.130 - mae: 2553.676 - mean_q: 3706.718 - mean_eps: 0.376\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: -1.0678\n",
      "1240 episodes - episode_reward: -8.611 [-35.000, 1.000] - loss: 264670446.787 - mae: 5847.688 - mean_q: 8708.483 - mean_eps: 0.327\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: -1.1551\n",
      "1208 episodes - episode_reward: -9.562 [-35.000, 1.000] - loss: 17660875.176 - mae: 3388.619 - mean_q: 4561.621 - mean_eps: 0.277\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: -1.0384\n",
      "1241 episodes - episode_reward: -8.367 [-35.000, 1.000] - loss: 1765132285.148 - mae: 19504.667 - mean_q: 17736.557 - mean_eps: 0.228\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: -1.0441\n",
      "1341 episodes - episode_reward: -7.786 [-35.000, 1.000] - loss: 151030478.087 - mae: 6084.488 - mean_q: 8741.932 - mean_eps: 0.178\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: -1.0801\n",
      "1310 episodes - episode_reward: -8.245 [-35.000, 1.000] - loss: 333186003.606 - mae: 10088.478 - mean_q: 14626.180 - mean_eps: 0.129\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      " 3894/10000 [==========>...................] - ETA: 2:18 - reward: -0.9342"
     ]
    }
   ],
   "source": [
    "save_name = \"bigger2\"\n",
    "env = make(\"connectx\", debug=False)\n",
    "trainer = env.train([\"random\", None])\n",
    "agent.fit(trainer, nb_steps=100000, visualize=False, verbose=1)\n",
    "agent.save_weights(f'{save_name}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-11T10:18:59.740644900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 730; L: 10; M: 260; O: 0\n"
     ]
    }
   ],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=1000)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T11:16:22.627208Z",
     "start_time": "2023-07-11T11:15:45.353632900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 712; L: 8; M: 280; O: 0\n"
     ]
    }
   ],
   "source": [
    "def kaggle_agent(observation, _):\n",
    "    return 1\n",
    "print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=1000)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T11:17:25.793357600Z",
     "start_time": "2023-07-11T11:16:53.066532100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
