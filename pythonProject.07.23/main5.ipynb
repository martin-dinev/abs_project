{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T16:17:28.056064800Z",
     "start_time": "2023-07-07T16:17:28.048556700Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor\n",
    "from keras.layers import Dense, Input, Reshape, Lambda, Concatenate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T16:17:28.229370600Z",
     "start_time": "2023-07-07T16:17:28.209346900Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectXProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return np.array( tuple(-1 if e == 2 else int(e) for e in observation['board']))\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, _reward):\n",
    "        return (0.1 if _reward == 1 else 0 if _reward == 0 else -0.1) if _reward is not None else -84\n",
    "\n",
    "    def process_action(self, action):\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T16:17:28.461115300Z",
     "start_time": "2023-07-07T16:17:28.361526100Z"
    }
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(1, 42))\n",
    "r = Reshape((42,))(i)\n",
    "y = Lambda(lambda e: e[:, :7])(r)\n",
    "x = Dense(42, activation='leaky_relu')(r)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dense(42, activation='leaky_relu')(x)\n",
    "x = Dense(7, activation='linear')(x)\n",
    "o = Concatenate()([x, y])\n",
    "o = Dense(7, activation='leaky_relu')(o)\n",
    "o = Dense(7, activation='linear')(o)\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T16:17:29.715385300Z",
     "start_time": "2023-07-07T16:17:29.039466800Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(0.1)\n",
    "processor = ConnectXProcessor()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=7, nb_steps_warmup=100, target_model_update=1e-2,\n",
    "                 processor=processor, enable_double_dqn=True, enable_dueling_network=True)\n",
    "agent.compile(optimizer=Adam(learning_rate=0.01), metrics=['mae'])\n",
    "# agent.load_weights('dqn_weights_23.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def mean_reward(_rewards):\n",
    "    _rewards = [r[0] for r in _rewards]\n",
    "    wins = sum(1 for r in _rewards if r == 1)\n",
    "    losses = sum(1 for r in _rewards if r == -1)\n",
    "    mistakes = sum(1 for r in _rewards if r is None)\n",
    "    opponent_mistakes = sum(1 for r in _rewards if r == 0)\n",
    "    return \"W: \" + str(wins) + \"; L: \" + str(losses) + \"; M: \" + str(mistakes) + \"; O: \" + str(opponent_mistakes)\n",
    "from kaggle_environments import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:17:29.790830600Z",
     "start_time": "2023-07-07T16:17:29.770028600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 142s 14ms/step - reward: -2.2642\n",
      "done, took 142.448 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: -1.2631\n",
      "done, took 143.446 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: -1.0360\n",
      "done, took 144.535 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: -1.0614\n",
      "done, took 148.023 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 155s 15ms/step - reward: -0.8424\n",
      "done, took 154.619 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: -0.6663\n",
      "done, took 148.735 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: -0.7252\n",
      "done, took 152.050 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 3070/10000 [========>.....................] - ETA: 1:45 - reward: -0.6523done, took 46.863 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   10/10000 [..............................] - ETA: 58s - reward: 0.0000e+00done, took 0.107 seconds\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    7/10000 [..............................] - ETA: 1:32 - reward: -12.0000  done, took 0.123 seconds\n"
     ]
    }
   ],
   "source": [
    "save_name = \"second_layer\"\n",
    "for c in range(0,10):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"random\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2-c*0.02)\n",
    "    if c>0:\n",
    "        agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=10000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:35:32.746296800Z",
     "start_time": "2023-07-07T16:17:30.830361400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 37; L: 0; M: 13; O: 0\n",
      "W: 33; L: 0; M: 17; O: 0\n",
      "W: 38; L: 2; M: 10; O: 0\n",
      "W: 41; L: 1; M: 8; O: 0\n",
      "W: 36; L: 1; M: 13; O: 0\n",
      "W: 36; L: 2; M: 12; O: 0\n",
      "W: 35; L: 0; M: 15; O: 0\n",
      "W: 41; L: 1; M: 8; O: 0\n",
      "W: 30; L: 1; M: 19; O: 0\n",
      "W: 37; L: 0; M: 13; O: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    save_name = \"second_layer\"\n",
    "    agent.load_weights(f'{save_name}_{i+1}.h5f')\n",
    "    def kaggle_agent(observation, _):\n",
    "        return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=50)))\n",
    "    # print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"negamax\"], num_episodes=20)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T16:35:59.431662Z",
     "start_time": "2023-07-07T16:35:41.514094800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Didn't execute below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_name = \"second_layer\"\n",
    "for c in range(10,20):\n",
    "    env = make(\"connectx\", debug=False)\n",
    "    trainer = env.train([\"negamax\", None])\n",
    "    agent.policy = EpsGreedyQPolicy(0.2-c*0.02)\n",
    "    agent.load_weights(f'{save_name}_{c}.h5f')\n",
    "    agent.fit(trainer, nb_steps=2000, visualize=False, verbose=1)\n",
    "    agent.save_weights(f'{save_name}_{c+1}.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10, 20):\n",
    "    save_name = \"second_layer\"\n",
    "    agent.load_weights(f'{save_name}_{20}.h5f')\n",
    "    def kaggle_agent(observation, _):\n",
    "        return processor.process_action(np.argmax(agent.forward(processor.process_observation(observation))))\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"random\"], num_episodes=20)))\n",
    "    print(mean_reward(evaluate(\"connectx\", [kaggle_agent, \"negamax\"], num_episodes=20)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
